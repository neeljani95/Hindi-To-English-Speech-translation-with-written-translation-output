{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17e4ef35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum length of Hindi Sentence  20\n",
      "maximum length of English Sentence  20\n",
      "WARNING:tensorflow:From C:\\Users\\NEELJ\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NEELJ\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, None, 300)            4209300   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, None, 300)            5262300   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 300),                721200    ['embedding[0][0]']           \n",
      "                              (None, 300),                                                        \n",
      "                              (None, 300)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, None, 300),          721200    ['embedding_1[0][0]',         \n",
      "                              (None, 300),                           'lstm[0][1]',                \n",
      "                              (None, 300)]                           'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 17541)          5279841   ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16193841 (61.77 MB)\n",
      "Trainable params: 16193841 (61.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NEELJ\\AppData\\Local\\Temp\\ipykernel_24864\\964026639.py:208: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\NEELJ\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "154/154 [==============================] - 221s 1s/step - loss: 6.9564 - val_loss: 6.3631\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - 195s 1s/step - loss: 6.3150 - val_loss: 6.3086\n",
      "Epoch 3/100\n",
      "154/154 [==============================] - 196s 1s/step - loss: 6.2640 - val_loss: 6.2784\n",
      "Epoch 4/100\n",
      "154/154 [==============================] - 194s 1s/step - loss: 6.2165 - val_loss: 6.2213\n",
      "Epoch 5/100\n",
      "154/154 [==============================] - 186s 1s/step - loss: 6.1461 - val_loss: 6.1615\n",
      "Epoch 6/100\n",
      "154/154 [==============================] - 186s 1s/step - loss: 6.0728 - val_loss: 6.1093\n",
      "Epoch 7/100\n",
      "154/154 [==============================] - 187s 1s/step - loss: 6.0102 - val_loss: 6.0455\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - 194s 1s/step - loss: 5.9375 - val_loss: 5.9801\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - 194s 1s/step - loss: 5.8686 - val_loss: 5.9370\n",
      "Epoch 10/100\n",
      "154/154 [==============================] - 195s 1s/step - loss: 5.8115 - val_loss: 5.8906\n",
      "Epoch 11/100\n",
      "154/154 [==============================] - 194s 1s/step - loss: 5.7577 - val_loss: 5.8525\n",
      "Epoch 12/100\n",
      "154/154 [==============================] - 196s 1s/step - loss: 5.7007 - val_loss: 5.8134\n",
      "Epoch 13/100\n",
      "154/154 [==============================] - 193s 1s/step - loss: 5.6446 - val_loss: 5.7590\n",
      "Epoch 14/100\n",
      "154/154 [==============================] - 187s 1s/step - loss: 5.5901 - val_loss: 5.7242\n",
      "Epoch 15/100\n",
      "154/154 [==============================] - 193s 1s/step - loss: 5.5415 - val_loss: 5.6950\n",
      "Epoch 16/100\n",
      "154/154 [==============================] - 189s 1s/step - loss: 5.4972 - val_loss: 5.6690\n",
      "Epoch 17/100\n",
      "154/154 [==============================] - 188s 1s/step - loss: 5.4510 - val_loss: 5.6355\n",
      "Epoch 18/100\n",
      "154/154 [==============================] - 192s 1s/step - loss: 5.4080 - val_loss: 5.6101\n",
      "Epoch 19/100\n",
      "154/154 [==============================] - 195s 1s/step - loss: 5.3650 - val_loss: 5.5777\n",
      "Epoch 20/100\n",
      "154/154 [==============================] - 193s 1s/step - loss: 5.3212 - val_loss: 5.5648\n",
      "Epoch 21/100\n",
      "154/154 [==============================] - 192s 1s/step - loss: 5.2773 - val_loss: 5.5316\n",
      "Epoch 22/100\n",
      "154/154 [==============================] - 193s 1s/step - loss: 5.2335 - val_loss: 5.5004\n",
      "Epoch 23/100\n",
      "154/154 [==============================] - 199s 1s/step - loss: 5.1901 - val_loss: 5.4800\n",
      "Epoch 24/100\n",
      "154/154 [==============================] - 196s 1s/step - loss: 5.1472 - val_loss: 5.4648\n",
      "Epoch 25/100\n",
      "154/154 [==============================] - 196s 1s/step - loss: 5.1068 - val_loss: 5.4338\n",
      "Epoch 26/100\n",
      "154/154 [==============================] - 195s 1s/step - loss: 5.0664 - val_loss: 5.4194\n",
      "Epoch 27/100\n",
      "154/154 [==============================] - 195s 1s/step - loss: 5.0270 - val_loss: 5.3990\n",
      "Epoch 28/100\n",
      "154/154 [==============================] - 195s 1s/step - loss: 4.9865 - val_loss: 5.3841\n",
      "Epoch 29/100\n",
      "154/154 [==============================] - 195s 1s/step - loss: 4.9488 - val_loss: 5.3755\n",
      "Epoch 30/100\n",
      "154/154 [==============================] - 197s 1s/step - loss: 4.9113 - val_loss: 5.3472\n",
      "Epoch 31/100\n",
      "154/154 [==============================] - 196s 1s/step - loss: 4.8733 - val_loss: 5.3552\n",
      "Epoch 32/100\n",
      "154/154 [==============================] - 194s 1s/step - loss: 4.8355 - val_loss: 5.3240\n",
      "Epoch 33/100\n",
      "154/154 [==============================] - 197s 1s/step - loss: 4.7973 - val_loss: 5.3298\n",
      "Epoch 34/100\n",
      "154/154 [==============================] - 194s 1s/step - loss: 4.7620 - val_loss: 5.3067\n",
      "Epoch 35/100\n",
      "154/154 [==============================] - 197s 1s/step - loss: 4.7270 - val_loss: 5.2904\n",
      "Epoch 36/100\n",
      "154/154 [==============================] - 183s 1s/step - loss: 4.6926 - val_loss: 5.2812\n",
      "Epoch 37/100\n",
      "154/154 [==============================] - 189s 1s/step - loss: 4.6587 - val_loss: 5.2715\n",
      "Epoch 38/100\n",
      "154/154 [==============================] - 189s 1s/step - loss: 4.6242 - val_loss: 5.2661\n",
      "Epoch 39/100\n",
      "154/154 [==============================] - 189s 1s/step - loss: 4.5907 - val_loss: 5.2552\n",
      "Epoch 40/100\n",
      "154/154 [==============================] - 191s 1s/step - loss: 4.5564 - val_loss: 5.2540\n",
      "Epoch 41/100\n",
      "154/154 [==============================] - 187s 1s/step - loss: 4.5233 - val_loss: 5.2537\n",
      "Epoch 42/100\n",
      "154/154 [==============================] - 187s 1s/step - loss: 4.4908 - val_loss: 5.2575\n",
      "Epoch 43/100\n",
      "154/154 [==============================] - 195s 1s/step - loss: 4.4585 - val_loss: 5.2322\n",
      "Epoch 44/100\n",
      "154/154 [==============================] - 196s 1s/step - loss: 4.4260 - val_loss: 5.2356\n",
      "Epoch 45/100\n",
      "154/154 [==============================] - 195s 1s/step - loss: 4.3946 - val_loss: 5.2351\n",
      "Epoch 46/100\n",
      "154/154 [==============================] - 194s 1s/step - loss: 4.3609 - val_loss: 5.2486\n",
      "Epoch 47/100\n",
      "154/154 [==============================] - 191s 1s/step - loss: 4.3309 - val_loss: 5.2304\n",
      "Epoch 48/100\n",
      "154/154 [==============================] - 193s 1s/step - loss: 4.2994 - val_loss: 5.2211\n",
      "Epoch 49/100\n",
      "154/154 [==============================] - 197s 1s/step - loss: 4.2684 - val_loss: 5.2341\n",
      "Epoch 50/100\n",
      "154/154 [==============================] - 200s 1s/step - loss: 4.2379 - val_loss: 5.2232\n",
      "Epoch 51/100\n",
      "154/154 [==============================] - 197s 1s/step - loss: 4.2084 - val_loss: 5.2137\n",
      "Epoch 52/100\n",
      "154/154 [==============================] - 196s 1s/step - loss: 4.1780 - val_loss: 5.2089\n",
      "Epoch 53/100\n",
      "154/154 [==============================] - 195s 1s/step - loss: 4.1463 - val_loss: 5.2216\n",
      "Epoch 54/100\n",
      "154/154 [==============================] - 194s 1s/step - loss: 4.1170 - val_loss: 5.2194\n",
      "Epoch 55/100\n",
      "154/154 [==============================] - 196s 1s/step - loss: 4.0884 - val_loss: 5.2177\n",
      "Epoch 56/100\n",
      "154/154 [==============================] - 198s 1s/step - loss: 4.0577 - val_loss: 5.2184\n",
      "Epoch 57/100\n",
      "154/154 [==============================] - 197s 1s/step - loss: 4.0286 - val_loss: 5.2212\n",
      "Epoch 58/100\n",
      "154/154 [==============================] - 197s 1s/step - loss: 4.0000 - val_loss: 5.2089\n",
      "Epoch 59/100\n",
      "154/154 [==============================] - 199s 1s/step - loss: 3.9711 - val_loss: 5.2252\n",
      "Epoch 60/100\n",
      "154/154 [==============================] - 200s 1s/step - loss: 3.9421 - val_loss: 5.2262\n",
      "Epoch 61/100\n",
      "154/154 [==============================] - 197s 1s/step - loss: 3.9128 - val_loss: 5.2270\n",
      "Epoch 62/100\n",
      "154/154 [==============================] - 195s 1s/step - loss: 3.8843 - val_loss: 5.2266\n",
      "Epoch 63/100\n",
      "154/154 [==============================] - 197s 1s/step - loss: 3.8561 - val_loss: 5.2275\n",
      "Epoch 64/100\n",
      "154/154 [==============================] - 200s 1s/step - loss: 3.8275 - val_loss: 5.2373\n",
      "Epoch 65/100\n",
      "154/154 [==============================] - 191s 1s/step - loss: 3.7999 - val_loss: 5.2579\n",
      "Epoch 66/100\n",
      "154/154 [==============================] - 186s 1s/step - loss: 3.7717 - val_loss: 5.2575\n",
      "Epoch 67/100\n",
      "154/154 [==============================] - 183s 1s/step - loss: 3.7424 - val_loss: 5.2316\n",
      "Epoch 68/100\n",
      "154/154 [==============================] - 184s 1s/step - loss: 3.7139 - val_loss: 5.2528\n",
      "Epoch 69/100\n",
      "154/154 [==============================] - 181s 1s/step - loss: 3.6872 - val_loss: 5.2552\n",
      "Epoch 70/100\n",
      "154/154 [==============================] - 185s 1s/step - loss: 3.6579 - val_loss: 5.2580\n",
      "Epoch 71/100\n",
      "154/154 [==============================] - 196s 1s/step - loss: 3.6300 - val_loss: 5.2691\n",
      "Epoch 72/100\n",
      "154/154 [==============================] - 189s 1s/step - loss: 3.6024 - val_loss: 5.2741\n",
      "Epoch 73/100\n",
      "154/154 [==============================] - 187s 1s/step - loss: 3.5739 - val_loss: 5.2761\n",
      "Epoch 74/100\n",
      "154/154 [==============================] - 190s 1s/step - loss: 3.5472 - val_loss: 5.2770\n",
      "Epoch 75/100\n",
      "154/154 [==============================] - 193s 1s/step - loss: 3.5180 - val_loss: 5.2958\n",
      "Epoch 76/100\n",
      "154/154 [==============================] - 190s 1s/step - loss: 3.4909 - val_loss: 5.2995\n",
      "Epoch 77/100\n",
      "154/154 [==============================] - 193s 1s/step - loss: 3.4627 - val_loss: 5.2894\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 187s 1s/step - loss: 3.4357 - val_loss: 5.3001\n",
      "Epoch 79/100\n",
      "154/154 [==============================] - 204s 1s/step - loss: 3.4085 - val_loss: 5.3079\n",
      "Epoch 80/100\n",
      "154/154 [==============================] - 212s 1s/step - loss: 3.3788 - val_loss: 5.3170\n",
      "Epoch 81/100\n",
      "154/154 [==============================] - 208s 1s/step - loss: 3.3512 - val_loss: 5.3334\n",
      "Epoch 82/100\n",
      "154/154 [==============================] - 214s 1s/step - loss: 3.3232 - val_loss: 5.3261\n",
      "Epoch 83/100\n",
      "154/154 [==============================] - 214s 1s/step - loss: 3.2953 - val_loss: 5.3540\n",
      "Epoch 84/100\n",
      "154/154 [==============================] - 204s 1s/step - loss: 3.2670 - val_loss: 5.3528\n",
      "Epoch 85/100\n",
      "154/154 [==============================] - 189s 1s/step - loss: 3.2403 - val_loss: 5.3560\n",
      "Epoch 86/100\n",
      "154/154 [==============================] - 188s 1s/step - loss: 3.2113 - val_loss: 5.3568\n",
      "Epoch 87/100\n",
      "154/154 [==============================] - 187s 1s/step - loss: 3.1825 - val_loss: 5.3707\n",
      "Epoch 88/100\n",
      "154/154 [==============================] - 190s 1s/step - loss: 3.1553 - val_loss: 5.3812\n",
      "Epoch 89/100\n",
      "154/154 [==============================] - 193s 1s/step - loss: 3.1265 - val_loss: 5.3952\n",
      "Epoch 90/100\n",
      "154/154 [==============================] - 201s 1s/step - loss: 3.0983 - val_loss: 5.4035\n",
      "Epoch 91/100\n",
      "154/154 [==============================] - 200s 1s/step - loss: 3.0720 - val_loss: 5.4054\n",
      "Epoch 92/100\n",
      "154/154 [==============================] - 188s 1s/step - loss: 3.0422 - val_loss: 5.4187\n",
      "Epoch 93/100\n",
      "154/154 [==============================] - 191s 1s/step - loss: 3.0156 - val_loss: 5.4378\n",
      "Epoch 94/100\n",
      "154/154 [==============================] - 195s 1s/step - loss: 2.9869 - val_loss: 5.4530\n",
      "Epoch 95/100\n",
      "154/154 [==============================] - 186s 1s/step - loss: 2.9595 - val_loss: 5.4623\n",
      "Epoch 96/100\n",
      "154/154 [==============================] - 194s 1s/step - loss: 2.9307 - val_loss: 5.4593\n",
      "Epoch 97/100\n",
      "154/154 [==============================] - 190s 1s/step - loss: 2.9041 - val_loss: 5.4685\n",
      "Epoch 98/100\n",
      "154/154 [==============================] - 191s 1s/step - loss: 2.8749 - val_loss: 5.5005\n",
      "Epoch 99/100\n",
      "154/154 [==============================] - 193s 1s/step - loss: 2.8473 - val_loss: 5.4956\n",
      "Epoch 100/100\n",
      "154/154 [==============================] - 196s 1s/step - loss: 2.8176 - val_loss: 5.5243\n",
      "Collecting SpeechRecognition\n",
      "  Obtaining dependency information for SpeechRecognition from https://files.pythonhosted.org/packages/73/8c/74d3b2a7d71e3f18e1e50bf3f168cf3333846137f5723efac3d0dc5a8635/SpeechRecognition-3.10.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading SpeechRecognition-3.10.1-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from SpeechRecognition) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2023.7.22)\n",
      "Downloading SpeechRecognition-3.10.1-py2.py3-none-any.whl (32.8 MB)\n",
      "   ---------------------------------------- 0.0/32.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/32.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/32.8 MB 435.7 kB/s eta 0:01:16\n",
      "   ---------------------------------------- 0.0/32.8 MB 281.8 kB/s eta 0:01:57\n",
      "   ---------------------------------------- 0.1/32.8 MB 365.7 kB/s eta 0:01:30\n",
      "   ---------------------------------------- 0.1/32.8 MB 438.1 kB/s eta 0:01:15\n",
      "   ---------------------------------------- 0.1/32.8 MB 450.6 kB/s eta 0:01:13\n",
      "   ---------------------------------------- 0.2/32.8 MB 510.2 kB/s eta 0:01:05\n",
      "   ---------------------------------------- 0.2/32.8 MB 537.4 kB/s eta 0:01:01\n",
      "   ---------------------------------------- 0.2/32.8 MB 498.9 kB/s eta 0:01:06\n",
      "   ---------------------------------------- 0.2/32.8 MB 533.8 kB/s eta 0:01:02\n",
      "   ---------------------------------------- 0.3/32.8 MB 571.2 kB/s eta 0:00:57\n",
      "   ---------------------------------------- 0.3/32.8 MB 571.2 kB/s eta 0:00:57\n",
      "   ---------------------------------------- 0.3/32.8 MB 546.6 kB/s eta 0:01:00\n",
      "   ---------------------------------------- 0.4/32.8 MB 571.7 kB/s eta 0:00:57\n",
      "   ---------------------------------------- 0.4/32.8 MB 576.0 kB/s eta 0:00:57\n",
      "   ---------------------------------------- 0.4/32.8 MB 593.2 kB/s eta 0:00:55\n",
      "    --------------------------------------- 0.4/32.8 MB 584.7 kB/s eta 0:00:56\n",
      "    --------------------------------------- 0.5/32.8 MB 589.0 kB/s eta 0:00:55\n",
      "    --------------------------------------- 0.5/32.8 MB 603.9 kB/s eta 0:00:54\n",
      "    --------------------------------------- 0.5/32.8 MB 608.5 kB/s eta 0:00:54\n",
      "    --------------------------------------- 0.6/32.8 MB 633.1 kB/s eta 0:00:51\n",
      "    --------------------------------------- 0.6/32.8 MB 644.7 kB/s eta 0:00:50\n",
      "    --------------------------------------- 0.7/32.8 MB 645.7 kB/s eta 0:00:50\n",
      "    --------------------------------------- 0.7/32.8 MB 665.3 kB/s eta 0:00:49\n",
      "    --------------------------------------- 0.8/32.8 MB 683.3 kB/s eta 0:00:47\n",
      "    --------------------------------------- 0.8/32.8 MB 699.8 kB/s eta 0:00:46\n",
      "   - -------------------------------------- 0.9/32.8 MB 715.0 kB/s eta 0:00:45\n",
      "   - -------------------------------------- 0.9/32.8 MB 730.2 kB/s eta 0:00:44\n",
      "   - -------------------------------------- 1.0/32.8 MB 725.9 kB/s eta 0:00:44\n",
      "   - -------------------------------------- 1.0/32.8 MB 730.9 kB/s eta 0:00:44\n",
      "   - -------------------------------------- 1.0/32.8 MB 744.1 kB/s eta 0:00:43\n",
      "   - -------------------------------------- 1.1/32.8 MB 748.2 kB/s eta 0:00:43\n",
      "   - -------------------------------------- 1.1/32.8 MB 759.0 kB/s eta 0:00:42\n",
      "   - -------------------------------------- 1.2/32.8 MB 763.8 kB/s eta 0:00:42\n",
      "   - -------------------------------------- 1.2/32.8 MB 765.7 kB/s eta 0:00:42\n",
      "   - -------------------------------------- 1.3/32.8 MB 782.8 kB/s eta 0:00:41\n",
      "   - -------------------------------------- 1.3/32.8 MB 791.5 kB/s eta 0:00:40\n",
      "   - -------------------------------------- 1.4/32.8 MB 792.7 kB/s eta 0:00:40\n",
      "   - -------------------------------------- 1.4/32.8 MB 801.6 kB/s eta 0:00:40\n",
      "   - -------------------------------------- 1.5/32.8 MB 820.6 kB/s eta 0:00:39\n",
      "   - -------------------------------------- 1.6/32.8 MB 831.8 kB/s eta 0:00:38\n",
      "   - -------------------------------------- 1.6/32.8 MB 841.0 kB/s eta 0:00:38\n",
      "   - -------------------------------------- 1.6/32.8 MB 847.1 kB/s eta 0:00:37\n",
      "   -- ------------------------------------- 1.7/32.8 MB 851.6 kB/s eta 0:00:37\n",
      "   -- ------------------------------------- 1.8/32.8 MB 867.2 kB/s eta 0:00:36\n",
      "   -- ------------------------------------- 1.8/32.8 MB 877.2 kB/s eta 0:00:36\n",
      "   -- ------------------------------------- 1.9/32.8 MB 886.7 kB/s eta 0:00:35\n",
      "   -- ------------------------------------- 1.9/32.8 MB 892.9 kB/s eta 0:00:35\n",
      "   -- ------------------------------------- 2.0/32.8 MB 895.7 kB/s eta 0:00:35\n",
      "   -- ------------------------------------- 2.1/32.8 MB 910.2 kB/s eta 0:00:34\n",
      "   -- ------------------------------------- 2.1/32.8 MB 922.8 kB/s eta 0:00:34\n",
      "   -- ------------------------------------- 2.2/32.8 MB 932.5 kB/s eta 0:00:33\n",
      "   -- ------------------------------------- 2.3/32.8 MB 948.5 kB/s eta 0:00:33\n",
      "   -- ------------------------------------- 2.3/32.8 MB 958.0 kB/s eta 0:00:32\n",
      "   -- ------------------------------------- 2.4/32.8 MB 972.9 kB/s eta 0:00:32\n",
      "   --- ------------------------------------ 2.5/32.8 MB 985.3 kB/s eta 0:00:31\n",
      "   --- ------------------------------------ 2.6/32.8 MB 999.2 kB/s eta 0:00:31\n",
      "   --- ------------------------------------ 2.6/32.8 MB 1.0 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 2.7/32.8 MB 1.0 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 2.8/32.8 MB 1.0 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 2.9/32.8 MB 1.0 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 3.0/32.8 MB 1.1 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 3.0/32.8 MB 1.1 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 3.1/32.8 MB 1.1 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 3.2/32.8 MB 1.1 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 3.3/32.8 MB 1.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 3.4/32.8 MB 1.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 3.4/32.8 MB 1.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 3.5/32.8 MB 1.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 3.6/32.8 MB 1.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 3.7/32.8 MB 1.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 3.8/32.8 MB 1.2 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 3.9/32.8 MB 1.2 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 4.0/32.8 MB 1.2 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 4.1/32.8 MB 1.2 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 4.2/32.8 MB 1.2 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 4.3/32.8 MB 1.2 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 4.4/32.8 MB 1.2 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 4.5/32.8 MB 1.3 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 4.6/32.8 MB 1.3 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 4.8/32.8 MB 1.3 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 4.9/32.8 MB 1.3 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 5.1/32.8 MB 1.3 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 5.2/32.8 MB 1.4 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 5.3/32.8 MB 1.4 MB/s eta 0:00:20\n",
      "   ------ --------------------------------- 5.5/32.8 MB 1.4 MB/s eta 0:00:20\n",
      "   ------ --------------------------------- 5.6/32.8 MB 1.4 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 5.8/32.8 MB 1.4 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 5.9/32.8 MB 1.5 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 6.0/32.8 MB 1.5 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 6.2/32.8 MB 1.5 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 6.3/32.8 MB 1.5 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 6.5/32.8 MB 1.5 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 6.6/32.8 MB 1.5 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 6.7/32.8 MB 1.5 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 6.8/32.8 MB 1.6 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 6.9/32.8 MB 1.6 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 7.0/32.8 MB 1.6 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 7.1/32.8 MB 1.6 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 7.3/32.8 MB 1.6 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 7.4/32.8 MB 1.6 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 7.5/32.8 MB 1.6 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 7.6/32.8 MB 1.6 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 7.8/32.8 MB 1.6 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 7.9/32.8 MB 1.6 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 8.0/32.8 MB 1.7 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 8.2/32.8 MB 1.7 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 8.3/32.8 MB 1.7 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 8.4/32.8 MB 1.7 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 8.6/32.8 MB 1.7 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 8.8/32.8 MB 1.7 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 9.0/32.8 MB 1.8 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 9.2/32.8 MB 1.8 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 9.3/32.8 MB 1.8 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 9.5/32.8 MB 1.8 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 9.6/32.8 MB 1.8 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 9.8/32.8 MB 1.8 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 10.0/32.8 MB 1.9 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 10.2/32.8 MB 1.9 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 10.4/32.8 MB 2.0 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 10.5/32.8 MB 2.0 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 10.7/32.8 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 10.9/32.8 MB 2.2 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 11.1/32.8 MB 2.3 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 11.2/32.8 MB 2.3 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 11.4/32.8 MB 2.4 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 11.6/32.8 MB 2.5 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 11.8/32.8 MB 2.6 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 12.0/32.8 MB 2.6 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 12.2/32.8 MB 2.7 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 12.4/32.8 MB 2.8 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 12.6/32.8 MB 2.8 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 12.8/32.8 MB 2.9 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 13.0/32.8 MB 3.0 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 13.2/32.8 MB 3.0 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 13.5/32.8 MB 3.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 13.7/32.8 MB 3.2 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 13.9/32.8 MB 3.3 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 14.2/32.8 MB 3.3 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 14.4/32.8 MB 3.4 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 14.6/32.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 14.8/32.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 15.1/32.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 15.3/32.8 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 15.5/32.8 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 15.6/32.8 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 15.6/32.8 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 15.6/32.8 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 15.6/32.8 MB 3.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 16.7/32.8 MB 3.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 16.7/32.8 MB 3.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 16.8/32.8 MB 3.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 17.0/32.8 MB 3.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 17.2/32.8 MB 3.9 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 17.4/32.8 MB 3.9 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 17.6/32.8 MB 3.9 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 17.8/32.8 MB 4.0 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 18.0/32.8 MB 4.0 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 18.2/32.8 MB 4.0 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 18.4/32.8 MB 4.1 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 18.6/32.8 MB 4.1 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 18.8/32.8 MB 4.1 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 19.0/32.8 MB 4.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 19.2/32.8 MB 4.1 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 19.4/32.8 MB 4.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 19.6/32.8 MB 4.2 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 19.8/32.8 MB 4.2 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 20.0/32.8 MB 4.2 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 20.2/32.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 20.4/32.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 20.6/32.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 20.8/32.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 21.0/32.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 21.2/32.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 21.3/32.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 21.3/32.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 21.3/32.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 21.3/32.8 MB 4.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 21.6/32.8 MB 4.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 21.8/32.8 MB 4.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 22.0/32.8 MB 4.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 22.6/32.8 MB 4.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 22.8/32.8 MB 4.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 22.9/32.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 23.1/32.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 23.3/32.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 23.5/32.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 23.7/32.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 23.9/32.8 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 24.1/32.8 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 24.3/32.8 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 24.5/32.8 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 24.6/32.8 MB 4.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 24.8/32.8 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 24.9/32.8 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 25.1/32.8 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 25.2/32.8 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 25.4/32.8 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 25.5/32.8 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 25.6/32.8 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 25.8/32.8 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 25.9/32.8 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 26.1/32.8 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 26.3/32.8 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 26.4/32.8 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 26.6/32.8 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 26.8/32.8 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 26.9/32.8 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 27.1/32.8 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 27.2/32.8 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 27.4/32.8 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 27.5/32.8 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 27.7/32.8 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 27.9/32.8 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 28.1/32.8 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 28.3/32.8 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 28.5/32.8 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 28.7/32.8 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 28.9/32.8 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 29.1/32.8 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 29.3/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 29.5/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 29.6/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 29.8/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 30.0/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 30.2/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 30.4/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 30.5/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 30.7/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 30.8/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 31.0/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 31.1/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 31.1/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 31.1/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 31.1/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 31.2/32.8 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 31.3/32.8 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 31.9/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 32.0/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.1/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.2/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.3/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.4/32.8 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.5/32.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.7/32.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.8/32.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.8/32.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 32.8/32.8 MB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: SpeechRecognition\n",
      "Successfully installed SpeechRecognition-3.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gtts\n",
      "  Obtaining dependency information for gtts from https://files.pythonhosted.org/packages/a7/ef/190f64a4edeb13165e3c598a08f06a2ae80cdae0aa208c96c20efdb7ad4b/gTTS-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading gTTS-2.4.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from gtts) (2.31.0)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from gtts) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from click<8.2,>=7.1->gtts) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (2023.7.22)\n",
      "Downloading gTTS-2.4.0-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: gtts\n",
      "Successfully installed gtts-2.4.0\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018FCEBCC360> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018FCEBCC360> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018FCED12DE0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018FCED12DE0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Input English sentence: and authenticity\n",
      "Actual Hindi Translation:  और authenticity प्रमाणिकता \n",
      "Model Hindi Translation:  और फ़िर \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Input English sentence: today food aid our food aid\n",
      "Actual Hindi Translation:  आज खाद्य सहायता हमारी खाद्य सहायता \n",
      "Model Hindi Translation:  आज हमारे पास हमारे पास उपयोग कर रहे हैं \n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Input English sentence: and clean it and filter it\n",
      "Actual Hindi Translation:  और उन्हें साफ़ कर के और छान कर के \n",
      "Model Hindi Translation:  और उसे और अधिक से अधिक होने का \n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Input English sentence: i talked about it a little at oxford two years ago\n",
      "Actual Hindi Translation:  मैनें इसके बारे में ऑक्सफ़ार्ड में बात की थी करीब दो साल पहले \n",
      "Model Hindi Translation:  मैं एक बात बात कर के पहले से एक साल पहले ही सा\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Input English sentence: at birmingham childrens hospital\n",
      "Actual Hindi Translation:  बर्मिंघम में बच्चों के अस्पताल में \n",
      "Model Hindi Translation:  पांच साल तक \n",
      "<class 'keras.src.engine.functional.Functional'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 335\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(decoder_model))\n\u001b[1;32m--> 335\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(decoded_sentence, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/saved_model\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import csv\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "lines=pd.read_csv(\"Hindi_English_Truncated_Corpus.csv\")\n",
    "\n",
    "lines['source'].value_counts()\n",
    "\n",
    "lines=lines[lines['source']=='ted']\n",
    "\n",
    "lines.head(20)\n",
    "\n",
    "pd.isnull(lines).sum()\n",
    "\n",
    "lines=lines[~pd.isnull(lines['english_sentence'])]\n",
    "\n",
    "lines.drop_duplicates(inplace=True)\n",
    "\n",
    " ### Let us pick any 25000 rows from the dataset.\n",
    "\n",
    "lines=lines.sample(n=25000,random_state=42)\n",
    "lines.shape\n",
    "\n",
    "# Lowercase all characters\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: x.lower())\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "# Remove all the special characters\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Remove all numbers from text\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
    "\n",
    "# Remove extra spaces\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: x.strip())\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.strip())\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x : 'START_ '+ x + ' _END')\n",
    "\n",
    "lines.head()\n",
    "\n",
    "### Get English and Hindi Vocabulary\n",
    "all_eng_words=set()\n",
    "for eng in lines['english_sentence']:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "all_hindi_words=set()\n",
    "for hin in lines['hindi_sentence']:\n",
    "    for word in hin.split():\n",
    "        if word not in all_hindi_words:\n",
    "            all_hindi_words.add(word)\n",
    "\n",
    "len(all_eng_words)\n",
    "\n",
    "len(all_hindi_words)\n",
    "\n",
    "lines['length_eng_sentence']=lines['english_sentence'].apply(lambda x:len(x.split(\" \")))\n",
    "lines['length_hin_sentence']=lines['hindi_sentence'].apply(lambda x:len(x.split(\" \")))\n",
    "\n",
    "lines.head()\n",
    "\n",
    "lines[lines['length_eng_sentence']>30].shape\n",
    "\n",
    "lines=lines[lines['length_eng_sentence']<=20]\n",
    "lines=lines[lines['length_hin_sentence']<=20]\n",
    "\n",
    "lines.shape\n",
    "\n",
    "print(\"maximum length of Hindi Sentence \",max(lines['length_hin_sentence']))\n",
    "print(\"maximum length of English Sentence \",max(lines['length_eng_sentence']))\n",
    "\n",
    "max_length_src=max(lines['length_hin_sentence'])\n",
    "max_length_tar=max(lines['length_eng_sentence'])\n",
    "\n",
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_hindi_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_hindi_words)\n",
    "num_encoder_tokens, num_decoder_tokens\n",
    "\n",
    "num_decoder_tokens += 1 #for zero padding\n",
    "num_encoder_tokens = num_encoder_tokens + 1 #he forgot about this one he must have put it but deleted it by mistake\n",
    "\n",
    "\n",
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n",
    "\n",
    "lines = shuffle(lines)\n",
    "lines.head(10)\n",
    "\n",
    "### Split the data into train and test\n",
    "\n",
    "X, y = lines['english_sentence'], lines['hindi_sentence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=42)\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "### Let us save this data\n",
    "\n",
    "X_train.to_pickle('X_train.pkl')\n",
    "X_test.to_pickle('X_test.pkl')\n",
    "\n",
    "\n",
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "\n",
    "### Encoder-Decoder Architecture\n",
    "\n",
    "latent_dim=300\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples//batch_size)\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba6e310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\users\\neelj\\anaconda3\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from SpeechRecognition) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2023.7.22)\n",
      "Requirement already satisfied: gtts in c:\\users\\neelj\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from gtts) (2.31.0)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from gtts) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from click<8.2,>=7.1->gtts) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\neelj\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "model.save_weights('nmt_weights.h5')\n",
    "\n",
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1\n",
    "\n",
    "\n",
    "!pip install SpeechRecognition\n",
    "!pip install gtts\n",
    "\n",
    "from gtts import gTTS\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Function to convert text to speech\n",
    "def text_to_speech(text, lang='hi', slow=False):\n",
    "    tts = gTTS(text=text, lang=lang, slow=slow)\n",
    "    tts.save(\"output.mp3\")\n",
    "    return \"output.mp3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "466d4966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018FCE5AE020> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018FCE5AE020> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018FCD522200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018FCD522200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Input English sentence: and authenticity\n",
      "Actual Hindi Translation:  और authenticity प्रमाणिकता \n",
      "Model Hindi Translation:  और फ़िर \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/mpeg;base64,//NExAAAAANIAUAAAP8xwUUwDMqb/afIYlGP+G5fap+CHOQW+RGdh7E+Qh9WKIk//crf/Odr7JjP/yfh6NK6lZP/yM3/QomxyUqQn//7/P/HkKeWPnUxGVehQxyDgNGj//NExFMSIvpIAZkoAWtL4thVU3z1Q33wVg+5MudR4Mbd9U8Pe5S9003uf1X5trNUfmPXPgavA8TWcbvDj1vO51tr519ZxvFMp+NEZFPHtr138fOtRs++sbw2bzqBErm+//NExF0g2mpIAZl4ATG97zi17fVoE9sYtL7Y1akKBEt9TZLwG1DpSZH++47K5381yUx06mhyGiMSJWiHTaCyEF0QzIgDgEJ+JhcLLltAguJ0IQd4DRQIQ5cAGcAbaMAo//NExCwcQd6EAZqgACCKHkFksVHQBsfMyMJ+mqzecUZKrUtSKFa6aVS2WtM3XZzA0NjY8YllIfBogamheIsggt+6jRFV2NWl2boKK80D31/SBpolNKQaGnCiXBCLpEGo//NExA4V4X6wAZiYAAXrBK4HH/lVInolmm4H+BYIWfAeMMKhZgIamQBohOhrjXNlOHLH1Pruje6rfqT9Zj7KLqrsbGiSeuuhrqNXeACrmxWfeAhD+r6K5q1VKgJO830U//NExAkTYXqkAdlQAJMMBmGoyaLRlwD2vY82umAmkCgdTZ0wNLA2oPEDiBFtcXDd6IR+e/b19GfRjHNIBQjpc/nLVDT0QfjwmQ7JapFVohCRmuauzRAeFkoi/dK3U5G3//NExA4WEYagAM6KlL6nEyjH9C1ktpJYIxwcjl9nNVUiWxeU0FO9LJ87fHRdfLmvi/f/dQnMKdH0R9gkGYkgjaVjvYoxqIdxwgLOBGbsvRFVnCR8PH2Gauf+cpOQm1bt//NExAgT6X6oAM4KlEBBeh2JNfvBdgMipzG4U5QjI2li3QMOZdZzuQ0uaDu7vSR/5/v6yw//EUbK/RyEYpHuJB4XnsnS3Yhzqdw4GjwwQbqq1ZotXct5VXSM/wIM/CJi//NExAsRkS6kAM5KcDEO99AfrjIjJ2AV722ZfACvm83f7ALiyKkpMACAULmMMH6q5MpU7M70CQRHC7avaME8+kYOuaBfEoKmVcJVvJroNYG+GtRebWGWKYwrp7KS+yUb//NExBcSATKUAM4YcbjbXcZa8rrT1apOt2itWOiIXAKBHTS2rP7xLaOUf2Yi5jkNmq0rR6bX6Z/PteCqY5Xs9QvvBgIxCYbTL0tI+bOA25z37cuAK01KsfiUPVLd+pSw//NExCISKSaMAM4eccz2MMC6BLTxVTlBSSFR2qlcxbVzI/Vj+NTD7M290vaPmuYL+PFU0lyv0u51ouqqWZV6qs/EWFUHDABj1W4VnKna0urZby7S8tS6MXX6KgFUWJEr//NExCwRuSZ4AMZecTuNl69o+9vb1rh89161ivXqtzq1r11l6+CrNVAyQ6ET3RFxZi1ViMyNUgxjdpPibX9/uI1U3Qvw/mFkjRb6tr515duAaLUKHAweDp045tNC3oc1//NExDgSWLpYAMPeTA7hGXQJ9jOv7vYpz+72XzFlargCo4kmmnVxBVMmLTgDAgBA8cg4kIiY8VFnrRWnXquSQIXvOCMiFxQuFxcGHOk2NPCIXFiUy76e5dltKUetvahZ//NExEERIG5kAHmEKBGlWLqW6THrDVk6CMXNAg5xFxApTKczhQ3fZpqVi01M0w2Hphs7geWSixjbucU9iERKyc+v8ivcjK4QQWHPlz5NzFBg0lDmX+u5L52Ggqsr3HXq//NExE8RQSaAAH4EcHeM9xbaRo0hPM3cDCFgBCMkDDifKljGx0QQB9ILnaSEbz3dPtD6Ff3E2ioHCqHDkKbOC4DLT/1MK5RW1Ms5CV6R43gUpB58HRGoQagKUKxPC7AE//NExF0QuIqYAMYwTPikBwE2IoBhQAoxZcLw1T66JxtGm99///5KKEEGWz87MSYgNjsO/bqZQqeit4RMjVyuOy1RUo/y+67Yy9I6ZZK5scJIpIdkjc05hIl5puPMJeWp//NExG0Q6XKgAMzElG9dhVbfcJ/Wet97/z/E3V7RPaPVigSnvnwjxdQpytDoHIqc8OPbqytzWQIlFWIskm1pAgBPHRgwMRVJAUxYJNpNEWW5kAGYo2nsPskicuRBMsLn//NExHwVWXKcANZQlGYkSigCZfKV5KZDINNiflsbTVorOOMtz1ays2tf+W//61pn5aL92fkTCyQkITi+bhRHJyubCTEQlkqPfM5T7nNx1d42fhTzhLlB7tgR4dFmSfv7//NExHkeqe6AAN6MmcsqBgShWYMDmDjhs84Z2hmsB4iDTDi0zMuAoqY0Gl4UAIIDzwtqD6zz0JhR9yYgu5mLJaay4Ujon+n4dj1nOrhWtd/PK7Z3//+O+/n7VnfJnGCi//NExFEgciJkAN4MmM1CmSfm5X7+qrtVNXeYjfzVO8v9nP3b95xvFS27M4xuuREVBE7Fau1kjFQ06u47Mj4hUh4gup2RsCYrdhqAX1k1O/MZh29hfw7vWO8b2AyjKRqS//NExCIZCXZQAVgYAcFeE9c7RgYxLCagqREk4RQ/2w2UT41yyJgZ0w64tLlox6xu2aSJVUFS/qKZiDSaKZQmEww7N27I7YqKYbPyeIu189V+2dpqN4LFiVn7WFJLP/m8//NExBAVEyJ4AZgQAXvlc5zeQjEORhH+rhxwNyflPs6EKAAz/27ZzRgjCP+e+uQ4t0CADQD/9vU7rZnOLgAgQglzr//5GkY/sfyCQBl1kIAC1d0fZzPcUQTnumgYyQIr//NExA4VwlqoAYYoAAeCDJw+ZlcQRyoznx5HIIhRKM1jGK5nEBCZ0FaUZlRBSp2ujCxlod9a7szsepXoydM9bJrV6IfrqQWFIxtE5Jqqgk+sK6XkaBfCmziINGXTkVN7//NExAoUqTKkAdgwALb/012Z1doNU9SV8p7OetN/T7nee9vztaSiR3wGIuaVjIK3PGBzIJlYglK0SAuaiY2QSHhANBs8UFTjjpF5P5iDZxb2PfRu/1rPu7g5Ej+39FlA//NExAoR2ZqsAMLMlCggbSW/fCsHZc2c3PzFPp8d121tfTorY/jPlpxUPolEly72jWlAmh9b+P2z//W/ZmzHdoU5ZNH8T1n6zcQV/e8m6gOSpvmMKDLJDlTjhA2BfDhq//NExBUTwZawAMtSlJD6S7qsWeibukkZN5dPoLdoSJQsuulGULJECmUmujS5g+pm37n7us9XnnWxrMztL29b6PbWxhQ1ExLees30DdNWkd7F4AN25z3FjBEGWZjQPs1R//NExBkR0Y6wAMQQlP+75kbuumgaNkMCOzucaGvQqUF7w9EQe5wgh9yf209NF900fXX0Ys0xyW/xb0KV/t2YaKVK4cHmruMkAyqWbvsREBRLieOod6dajJ/b0TC2o6/b//NExCQSqV6sANQWlMbTSmyoPU9oqw+5eSgyQ88fUumRnf7VGxEl6w2Hzzxor6zNt2ZZQqnz8CJXHujBjJnUhgwZBiz0Syy6homRJDPZR87YgbLeXeVu99dv+v+Ji6KF//NExCwSMTKcANYQcJVZsOgXHciosdNciprNZIeuqpb63NQCoK//iL5aaxSukyEQhAYh00MBAGADEqF/QuAYYJELvHTQKIoutSQ0FNSK5OTZlVep3QMmRHDQ7IOdGoH9//NExDYQaaKAAOtElba7l/0//+quHKM3nVMZc6jEwABAJtow4EsiApIiPMbBIehg0zFO5gBsHR3WmO4eS0mQUZqoIGP2pLNkB+A23KBOtQolDpkVtF7f//v8okzj3//6//NExEcSIaJ4AOtElH6VUTk7uuuAhodZqBnknCRDDgIo4mCYIA4QVcR4G4T3FM6mvbbxaZnLxQw4ouebfEDI4gPC4LCccik29m/r9dBtH///R//9KnnYuy0hABkVNWmw//NExFERWM5wAOPGcOfV301bNr13H9atfnWr2L9iU2QoCECqYmq0sM0619Z0SGFKJwRgJwpWv+ncaWpaBylJ2cui5mns0u0McqOuRUMVBDSycXha9NzXUO/+q2DsxgUi//NExF4SaPZkAVsYAHRZab9FNYy5NDMFD6mZNAxN3IAa/uyaao8ETIYRIcBN/2dBS7l0yIIQwmRmx1/9Ct6adyJk+NgXOTAFsRwOwc3/9aa1MzoKW6ZCiyA984xuOgiA//NExGchYyqEAY+AAFzBCHxm///VZ0FLuy6myLk+XC6T4uAsk+RQgBUQY3Vuc66agvD6L+zKQB0AwMjyX7zi15dV9t9M3qrc/8fvMZSvnz891xu8zFXH8Xft7KtQzT/M//NExDQcQrqwAc9AAU1jRIsmKVedtLEmiEMsHRMCph8jIEYPRQIpBqOYVqxh1qq1ZsitWowVoWcc3DQ2tQ5g+8kk+UaCymz8HfvDDHBnpqUT0QNaZkIVxGlf294+GqtH//NExBYR2aqoANHMlAXPsNvt7fN8DSmxwis8GBLOzkzlzAIRyr91///+///bP3dqNiFIJAMI8oBxKWMCUt0K7X7buHSKqWjLpbFTGTIJqJVd2uQsbZl5/T/4anX3/B98//NExCEaYhqQANvQmGc2181lZ8x54BwPWPCtaWSJQ8AYxDUgDA8KoHAFgaDNbElTKDTE2VHmd671JHpRqh6Hkqzu8XHja21aNiTJPGGjzb9P//9tuYrG9qZd85Jpyr9E//NExAoUMS6kANHMcNhAo02gUFjIcIzvRXmMr7VzGtp/9mV/ggKubcsv0xRKqz06lfV0nqmV3fceQHzZdKOyEXukEvqde4ocOCxz6j+opOPFThCqO4SQQRCvKgDQpTtW//NExAwU8v60AFBOueXmf//+H5WX99//pv//2f0U3s6+p6rdkKGqhhjMg8a9T+xjos9zXIDjniOXNJzxwxCrlnKg7PPPHCAoGDDYP0YblyyiiCkuPq6////////+X/X///NExAsVaxq0AAhSvV///r/3f///9X/6y/Vf74VmbPvz768Zs35oO5AnPIqtMMTrCQjWEZXIikmTaSRYrJOb6LEwIoS2IlEIpF1iyKC7ZxAqwjJkYsy3VQf/////+Z0m//NExAgQUx7AAAgKvPXsyvoj1aRzmPZR49pmK4mOU5RyKNVhUoCPMhAOJAxQ+FFKFqDjBgFDoRHFFhAoGHlBYDIIqAxA+dV//////////n//v/////u///t/3fPm9/+///NExBkSox7AAAhMvG/+f5+3ptacxi+fHlIWT7G0W6XiFnOfEems1yJM9UD0yBw5aXRg/Vbd4iXhRqQwigplnJkrgBhWvfcKYN1d/v9P5f/fyA//5/16f/dftr9JquVK//NExCESExK8AGBEuf/9q91KUqBUWrXZJjGUBKUz5S1Zys7lZUCiTOrFLYqBgwp6Fo9BQ1WrrKhdg2BpBaCqWcMTNAzIH6kMUSpbtX50o0nSW0dSkMxQYNFmexeo5I/0//NExCsR4RqkANNQcJqUxlWYODRJZpyv9KFMFC9J9aXWi6+pH///WmHF1vVVuRpSsvbDahRjIpQXFDsBsITcqK3lW6P0EVmoXFptAuAMc+NOgiIjShEteKx01b/6lQ+w//NExDYPySKoANKOcHkxOaCSVsYP9Sq5V7QhUiZ00ue7fEIEzsgWZ2NYA0ggjc84hEK2xM3T0FrxVZXKAuhXhrXtoYb7zNK6CKI3aw0T038t7dRMbLvsxTFTIGF0a5TA//NExEkRwaKkANKQlE/hfc3GDhIBpCJYEBHQwVSyIJwYkEkAeX3Wu9bettpmvT/6fO+VRxGEp7ft/Od587LEi1A1oiUFQFaoCkSwiioi6h4lWdff8jUYAdJgCfIhgMBk//NExFUSmSKAAMsMcDFU5pWgymbDz+zkzGaaj6HTyxGWLVpBsBFTpwjDvdNotsUqWVbBMsh7zyq16Gvdkui3bYv+zu6O4so0w8eDnTMlUOa885rHKV46doUbywysS+Ys//NExF0Q2E5UAVkAAJLEVrEdTlKqqYzEQOXFLtUV6HRRkoKV8Xv6k3Yf2KAvF5oy+qa6X2gXFx8uQKSzcRT9tV1xFg0waCkuQHgn6XZ5nhB9Jxfe8aig4sOxHBuH+fFk//NExGwgAxJMAZpAAU318vF3MfpPdr66xd/NohhAogBDZ4JrB5PeGAoiAD+eRj0f1TxMcrX+eOTSrHf/3DESfCvZ7/8QBWf0Di4iEwiI//+aIk9rjQ8Qi9mxSsWf//nH//NExD8ggyqkAYlYALTe0CZTF03G7XyO47////J8dhMOBvdW947A9D2CULwHRUIIdSg0EtT/////NDgezd5ufuuv/JQ/FxcUko+WvUgpNTREqmxcFlAqUFZiCLEyXgv4//NExBAUYgKsAdMYADEqVU/q//+X//Plupc9QEBJSIWHNoEEg3dAQMO9hG75FEnOQpIZVZHKT9dVX4e6qoCGMkWSWHjTuBgKnLBUNf52qrU7TRFHYEchj4JFZ6X0hhJG//NExBEWghqYANnWmJA94dA06uN/t6fR/R/KtdWJxJ9Q2AhOAiIHSUQxIPk06uQxSkpCI/1b9xzuouql9Ek3s9mk9z9f/s438/DdrUrDSXfR+7/fs5uzvCbXwYHTAqUj//NExAoTWZqcANlWlHYqQ2DBVIPDPCHoEPv/9/vf8/+9S25IFg9mw7isvKCxSCo1D7VVecX/pPl1QatftVQQckQN9YAHanhYyEtv///GZUC1nLlmjU0MjYgWkRiQclRg//NExA8Saa6UANnQlIMFA9v8NndBuvmv/1f0fd3Lf4IFCyIGmBcHDRZkB0FojUOGFPTXbcXH1+MOuKc8Peruxx+5xJLEJvzSyjEzKiAgMYqzcidbkE4poGTkwBfFM0TM//NExBgP4SaEANyMcMXKbItZJTp23s8vlPONVHJaajOPPdHAYdWvQlFqOsij6K+eBmNDQFpAWAgA7wOBhmV3HH04ZDCJgICA4Bmtx1cOpDG03y7+gh7jFjqqkCPEx6ms//NExCsQ2H5QAOYGSF7RKgrjC7hZirLRnsmyS3K/1/9X1f/3tRWAbqtfvgolanY8OfES8MxEYnOIIfcohZwkphCf1J5x8qpopkim8SumHN0L0uhlTicQ9510oATBBTnR//NExDoR6ZJgAHjGlDv9rzlaVz8pvEf/9TNCFSKBYcB2ZKXkIdGHb8ZOtOW19slP+dfc73ugmR3aMt0zJIQxY/1mxaemPRCCB6bEHukIMUnacGIFIQICiAI4W1hd6TjC//NExEUUGXaUAGGMlIAwb2jGZD98v/R9dZTNV6BhR0VNzeUlAJ1XAwcCsP0HJBnasn+vNLRuUk9Ebco2UfFa4EwpLpLsyExC5CVUuS7JZNDHHNawWWdUbhpbqYaV33VY//NExEcQ0TakAMIScESALZltiDhCiZtQ8VSvoMpEnU+F9uEO2d53Kff4VL1/tGBwZZUfIK+yLV4EC9MUePo016MM2ocB7ND7+SFXMd9KAIvV/eURCBJcxbNnpmyiLLJo//NExFYRwSKkAM4ecSxhoTjysXm3BDhpTbgBmjTqPUNwuK/WsO22pfxvm4IccTxhT0a7+TNd3xvPpusGs1skJHnqZ8VWA96V1zGAjxGf7VGWoVRaZTLN8BMKnok5jtMe//NExGISUP6oAMZece68aRUqrShCYmtbkqGa0n+hqUgwYRxSw2qFG081v4rbPzmv+669rMLj5JSjbmdA0GGrukGQIbeLXLpRyFpqJT+LDhHWI1aRRM1XTieW2sCFa0MR//NExGsSMQakAMZecFAD5PJQp0BBLjXDWLi5uEBQuPAx4oEHv/TR/JZYepqCW5AxFOmXAUCsOwA18qldkgrE7lfBsa9rOtNmouYCEp8KSPEAzNpdpEA/Ym7lvrbLXnXx//NExHUP6KqYANYeTBawDBU5cJy6P9vHM///////uIWUxBrZgiMfMjhgQ37dDYB1G2j9K8oDa0rLB/3qddfbrhiGNLZfocZlbwFMa4mbnRPZ5qNTMKLf5zN/JZRMVBNg//NExIgR6MaEAN5ecFf+XHsJuE///////6Opq0z+hdAdkaioiGShDGnkMkpUUhY4JAJcWkAtp3rokt0R1NENr0pSPamsdOh7UF3PoZ5eFBUkcQndWMb/c957O5JGSmt+//NExJMTcOaEAN4ecLKMVCORtfnO0TD4HFCej+r8cEZwOtCq18YLuT8ZbOxXGgHWQWE2GGCUk0SmRQ0/ZnFwfTJ1pgkx96y0ZvbeUcx5lqWc3+O1QY1//XulA0qo16HQ//NExJgVQW6YANZKlIFRMRHq8dY1hwZ2JFa1wZRPZUJYL2pWKjZMnG5CnKGuPlGnQNOIeAlVcYialO0z0MU3ymafhz4b1+q36DPv/+9nCMjgTKcKegEUWrAEa3oV5hbG//NExJYSMWKkAM5KlQAHuvx2ZHS717NW1T+Nchlc/GTEAhCG+kaX2SasktOUIXlT008tqDamPH8p+fO38dUGKUfRuvu1TO6iJrDUGIIupTBonj9FKufSozH1b3Wcwsh+//NExKASUW6kAMZElPLaTzZ7F8QErWpIqtoJOinZsUAQ3lcsQFL0rPYpkXbb+xTNxkeeVNY5ruGgtP0V5leqNU1rFV9VQMXV3e3bij5llP4YqvbbHNRxTne7AbntLbAl//NExKkSiWKkAM5KleHamLGB41m/VUuwpR+ApwUod7HxGDRSJbyZ575/j+YQI6iRxZUA3kyq1q21gfapAP3cMQHdeQOmI2Nt/EtlxZduKVQuW6ZkzbnY62s5ajHKLa7o//NExLERqVqoAMZElYcSk1WZvQOd4V2t+1f6/PzTdbSzfGYvjhx6IqqxHmQhU2OvgUE0lWwYy9DQs16Sr4KSSOxATEsa25FSX8a0Wy7JHDuZSmXZrSGsVMi6u3Lqn9YX//NExL0PyNqsAMYacda4zZBpgkoZ/+oa63/////T/yOW0McBqU9/xReAi+5h7aETFHKMMQBVDkFJDoYJyCFxhSryEPnzKhZerwIea1gRI2o0RhhNzCqzAvnzoD3iWT0z//NExNARcR6gANYecKw6wogVuN3EMjkg8oZx1f2NxvMbaitX+fuxzDjDm3cxzBXzNv////7suYWOTs8iqPiieEWyJj1du6UAi4sK5OUgWUpNCNlUBzj1RYG3rQ6f2SUu//NExN0SeOqEAN4YcJqUtVIhEKyx8FhGJ0SYw5exAsaCwIonUYhUKdk/lzjkMrpr4zaAgU0+TPl33H////RuWsbErEUAVhjU2tM30HPybgYxcZsKKhqc5USWmBCWoUF3//NExOYaoZaIANPYlDf//1OHnQRiYlCUJQklRMLUBcJTlTRJYis2de6aOc81Dp5g7Z7K//+jqiMUNJCOioy/yyTOyjhnCIUwkKATEBWLP4WyADIdSx0UmWXeZJakv/////NExM4WqZKUANwSlNWmM6xIgGQBg8LAMAQeKJSJ0rK4/wVIndmVESFPFRFI1VEh59KswkTjgKBrUvqQ7MR1/ZTh+N7etdwy1zIqWOUuhjdOxWQj8zaOJmBgILQaI0LZ//NExMYT4gaYAMnOmMax9OtVn6Spr1XphK+aT/3e5apVkgYyUkGF/mRyoVz3Nvk8F3tUgCuD2GUjTjnGHJcehNJgWwJ+3mjIIMTSVJhYS92RSppuggxqX0y+kPB0VetN//NExMkQySaEANNKcPpmBcRKBfIZn/00zRky4XDQvuPQd5GHiPhsSbfv8vm6DW3sgUy8XyUIxOMHJQrJNNS/19fQYvpuyFCgmmnlpgfMDhqXSagfUZgFCwxju9cxiMj2//NExNgRCQpYAVkQAAJRk9rNcZdmzVrWq1CgIlBRhQxhVLI+7N1UDKTCgKCs7gV4Klg6TX+GlmIFZiwjUOJXcPFSRbYlbGpJUWvlUPqpd1qUCMhSFzzPEjXlFG01b89c//NExOYiayJwAY9oAdeLHCkHVTQNFXCLWPU8NBQGipZZWj4iqZYoqdrFnnqjRERJliQUQu0ZWdPHqyybw1UBTquImrnVVRyR4uCmNwjcnikal9TV6/4EV7Yru1mPkVX1//NExK8TOOqMAcwYAN96TNIk0LaGTXGDCr1uWhqGVoreqqqkVO2CAgqk2KDzcot6IBPK4nOLi3QvW5F9ShWJrDetQZ5IKSbsUN23by/WFgm8DDyU4Lh8AADaTIRQ2Fj4//NExLURyIJgAHjGSOY1yzIgKuYddGIF3tsHvvvrUCAgRY5xGNdYLvj9qfq/K6Krde6OJbAUH7cw38+a1GfXAgScIgUEEBwDwKIANEwdtBBMYR3QBSRhDoTRN0NnN2PB//NExMASWO5IAMGGcHUaB3FMePTXMEE0KRKlAxLg5Sl/bQW8wRPF0wt9Ney0E3VUzGhTIJxBzVHQQZN7pugiu60HSWuiapJsxNoD1Sczb3/9NZ9+6ey30ScYH6jpgijo//NExMkRMEpMAVkYAKqrO3lAxYrsxFNZH1rSyi3KRTnqUBNSZcPFzwNeclsUe+Qyxrh4+RgbMTIx8IKgmHkZCCaJ2wj7+ef3w9fM+TlbDdgDd9Eu8unX3SRdXCCAwMh+//NExNchExKIAZloAWgyUKsCii+T3uibAF+rbYxoVqWKUW6jwM0l1dlZ/MzYJuhNXzxm3nFc7rr7/+P9f5xu8fFqRVO/1ZGP1ZoCOea6toaXaphQSfUpkiCezcQ5jXZY//NExKUSARqoAdhIAOkFINtEVsCxYRKWPs1uTK1CWNRxg7j6JspOmp/fX09FF0DVI2PvIvUvxazTOnfbVZbTdNIebZALWzgsWdW3sOFKmYHAycLuCJRb319iwnllCx0K//NExLARARqwAMYecF/aEuGnySI/AnJJEgQZ02PMtnXbo7PoskcNpmZGzXsx/JhR6oAwYSDBRt0BFpvrIyN8oyytANC2lkRkgVbAogv6wyCkIXfkqbjUHYh0v9C2uAjo//NExL8SGRKkAMYacXCMQqTdZqi96//2raq6aJulR9VFoocD6pDPLkE0X5TsJT4rVQto4aVzjkQFZq+yCAMC1xZAAC5qjSmLBYmOgW88b0VRSoZQqhxc7MY2bt/zjqXN//NExMkRwRagAMYacEJMEyp1dgQwjV1NMIIRqhbdguIPGWh+Xwwy5hQ8GwfoQgpmXKVpwNKYyKkR6QvWg3kCslgUuinK/OEnyj1eWYdqyneeOfK7aZm/Wa7lVJgOwUgt//NExNURmRqYAMYacCyCzRcfbxNujsTRxQ1X/oKsKJVyXMCpg4I6Nq3gYCly5b9Q9ZoYdlr9Mxy/m8zn2kwaKYQDAajAYrYwF/YquUONdstreKW9nqqHvVP9ZYMCAhQp//NExOEQ6R6QAM4OcAWZpmBnzZ0JN/p/FAuUiwmWPlKDqGosxWGoY2/0/8BRpS2l2MoYx+iGl5J2IgSc+0NUNSVPlOrnRfkOV0eDphe0riqq0tzpfka+q7Q2ZmMKJDC8//NExPAXIWp0ANYQlONcKQwHXbq2HdUsWfU+hdKVAEwVmEBUhEICBU8w5iVI5UW3DM6uaEoVAyCUdSEDIESAZr1ipKWQ6I4ETgx5p6F3JmvWYwEgI2b+k2f/7f/lGrVW//NExOYTAVJ0ANPElGqkBQUS1VD/rQqA4BslKh4bVr158wFAgWSbnihodQ8hh8lFryFtNpsprw46PHT11MY1O9Z7/kaZb+kSEWeKqP7Z/tc3tUzbhj1lh++C1X//6Pdm//NExO0V4WZgAMPGlANTqnX2ZS//2Z/dLA6H8cSoOxBRFY/Ol7qqHMRNBUQbTcM+2lzVpBXFQEMoJCYh1UdLP5nAPBAPItMF3H5uwdJ4/r58lChsvXmlSpJt386x3v3d//NExOgSsXI0AMMGlB8C5g5baj1FdbciS5Jdfb/33fYk5gEscIWDx6SCjXkukFhsw6Y6L96UfjnIfKcGyEwWYmmGhTQysnj7x+8M38AJNIFGaiHE+3dL33n/P/9f9+/q//NExPAVePn4AGGGce92va5b/17vtz/cZ+4KIDLtbUItOTBMGXc6WKz+NkonxclIEU0kCCBCy0jJ6cpvRltfSmmfL6paAdNCNCD8SmxuiPeERZw2uaHDDXWDzgXXHSkD//NExO0WcMXwAGGGcUU8SDZMOKmCTre77eT8SvPRvyHJ3M8n8D5GcjdU4HdJEUEDGGoDkbMuNwWGHi8uxBtrRAJwexGOalJQRHUn3RQa0wlAw8eDRsSrUwwU7ND+QizG//NExOYTMF3sAHmGKe91btHbvUxqEv1aE7GUvc1SMaUBegFQ0wl5o+zjwrFgtMV5aa7MMOgZ9Q1IiPgKBKjXIQyUBJ8VlwqaITMlSUsGl0V0RNkJmt1VDSJr+1mrViXG//NExOwZed3sAMGGmVIMK/CgNAS6QYVRR7OAiY3VgEKVeNQEmNQQkEBOsMKMFEw1gEYCaxgomN67N1f6pM1iwCcBRgowUSGHCoBOAuJgoKJDCgphUAkqIXUSYcpUF/Oh//NExNkQ6JnkAHoGTEbIxtgEBiCakpbadtJljn5qyy3zVqCFxUVFRUWFv/9Yq3UL/rFRQWaLCwqK//6hYVFYqKCwefWkBCzcVFVMQU1FMy4xMDBVVVVVVVVVTEFNRTMu//NExOgjWwXMAMpGuTEwMFVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVTEFNRTMu//NExK0Q8LE4AHmGTDEwMFVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV//NExKwAAANIAAAAAFVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV//NExKwAAANIAAAAAFVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\" type=\"audio/mpeg\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Input English sentence: today food aid our food aid\n",
      "Actual Hindi Translation:  आज खाद्य सहायता हमारी खाद्य सहायता \n",
      "Model Hindi Translation:  आज हमारे पास हमारे पास उपयोग कर रहे हैं \n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Input English sentence: and clean it and filter it\n",
      "Actual Hindi Translation:  और उन्हें साफ़ कर के और छान कर के \n",
      "Model Hindi Translation:  और उसे और अधिक से अधिक होने का \n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Input English sentence: i talked about it a little at oxford two years ago\n",
      "Actual Hindi Translation:  मैनें इसके बारे में ऑक्सफ़ार्ड में बात की थी करीब दो साल पहले \n",
      "Model Hindi Translation:  मैं एक बात बात कर के पहले से एक साल पहले ही सा\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Input English sentence: at birmingham childrens hospital\n",
      "Actual Hindi Translation:  बर्मिंघम में बच्चों के अस्पताल में \n",
      "Model Hindi Translation:  पांच साल तक \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "\n",
    "output_text = y_train[k:k+1].values[0][6:-4]\n",
    "print('Actual Hindi Translation:', output_text)\n",
    "print('Model Hindi Translation:', decoded_sentence[:-4])\n",
    "\n",
    "audio_file = text_to_speech(output_text)\n",
    "# Display the audio file in the notebook\n",
    "display(Audio(audio_file, autoplay=True))\n",
    "\n",
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Model Hindi Translation:', decoded_sentence[:-4])\n",
    "\n",
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Model Hindi Translation:', decoded_sentence[:-4])\n",
    "\n",
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Model Hindi Translation:', decoded_sentence[:-4])\n",
    "\n",
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Model Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15f5782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.src.engine.functional.Functional'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(decoder_model))\n\u001b[1;32m----> 4\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(decoded_sentence, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/saved_model\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "print(type(decoder_model))\n",
    "pickle.dump(decoded_sentence, '/content/saved_model','wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915aa5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
